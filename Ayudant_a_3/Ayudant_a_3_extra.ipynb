{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b8c60c",
   "metadata": {},
   "source": [
    "## Ayudant√≠a (3) Extra\n",
    "\n",
    "### Profesor: Luis Cossio\n",
    "### Ayudante: Gabriel D√≠az"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26dd111",
   "metadata": {},
   "source": [
    "# **Regresi√≥n Lineal con el Dataset del Titanic**  \n",
    "\n",
    "## **¬øQu√© es la Regresi√≥n Lineal?**  \n",
    "La **Regresi√≥n Lineal** es un m√©todo estad√≠stico que permite modelar la relaci√≥n entre una variable dependiente (objetivo) y una o m√°s variables independientes (predictoras). Se basa en la ecuaci√≥n:  \n",
    "\n",
    "$$ y = b_0 + b_1x_1 + b_2x_2 + \\dots + b_nx_n $$  \n",
    "\n",
    "Donde:  \n",
    "- \\( y \\) es la variable objetivo.  \n",
    "- $x_1, x_2, ... x_n $ son las variables predictoras.  \n",
    "- $ b_0 $ es el **intercepto** (valor de \\( y \\) cuando todas las \\( x \\) son 0).  \n",
    "- $ b_1, b_2, ..., b_n $ son los **coeficientes** de cada variable.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Preparaci√≥n de los Datos**  \n",
    "### **Carga del dataset y selecci√≥n de variables**  \n",
    "En este an√°lisis, usamos el dataset **Titanic**, que contiene informaci√≥n sobre los pasajeros, como la tarifa del boleto (`fare`), la edad (`age`), la clase del boleto (`pclass`), y el n√∫mero de familiares a bordo (`sibsp` y `parch`).  \n",
    "\n",
    "Para construir el modelo, seleccionamos las siguientes variables:  \n",
    "\n",
    "- **Variable objetivo:** `fare` (Tarifa del boleto)  \n",
    "- **Variables predictoras:**  \n",
    "  - `pclass` (Clase del boleto: 1, 2, 3)  \n",
    "  - `age` (Edad del pasajero)  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Divisi√≥n en Datos de Entrenamiento y Prueba**  \n",
    "Para evaluar el rendimiento del modelo, se divide el dataset en dos partes:  \n",
    "\n",
    "1. **Datos de Entrenamiento (80%)**: Se usan para ajustar el modelo.  \n",
    "2. **Datos de Prueba (20%)**: Se usan para evaluar qu√© tan bien predice el modelo en datos no vistos.  \n",
    "\n",
    "Esto se debe a que es un algoritmo supervisado, los datos de entrenamiento y de prueba.\n",
    "\n",
    "Usamos la funci√≥n `train_test_split()` de `sklearn.model_selection`, que separa aleatoriamente los datos en estas dos partes.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Creaci√≥n y Entrenamiento del Modelo**  \n",
    "1. Se crea una instancia del modelo de **Regresi√≥n Lineal** con `LinearRegression()`.  \n",
    "2. Se entrena el modelo con los datos de entrenamiento usando `.fit(X_train, y_train)`.  \n",
    "\n",
    "Durante el entrenamiento, el modelo **ajusta los coeficientes $b_1, b_2, \\dots $** para minimizar el error entre las predicciones y los valores reales.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Predicci√≥n y Evaluaci√≥n del Modelo**  \n",
    "Despu√©s del entrenamiento, usamos `.predict(X_test)` para hacer predicciones en los datos de prueba.  \n",
    "\n",
    "Para evaluar el modelo, calculamos:  \n",
    "- **Error Cuadr√°tico Medio (MSE):**  \n",
    "  \n",
    "  $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "  - Mide cu√°nto se desv√≠an las predicciones de los valores reales.  \n",
    "  - Valores menores indican mejor ajuste.  \n",
    "\n",
    "- **Coeficiente de Determinaci√≥n (\\( R^2 \\)):**  \n",
    "\n",
    "  $$ R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} $$ \n",
    "  - Indica qu√© porcentaje de la variabilidad en `fare` es explicado por las variables predictoras.  \n",
    "  - Valores cercanos a 1 indican un buen ajuste.  \n",
    "\n",
    "Dependiendo del tipo de problema, usamos diferentes m√©tricas de evaluaci√≥n. Para regresi√≥n es distinto que para clasificaci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Visualizaci√≥n de la Regresi√≥n**  \n",
    "Para interpretar los resultados, se grafica la relaci√≥n entre **Edad (`age`) y Tarifa (`fare`)**.  \n",
    "Se superpone la l√≠nea de **Regresi√≥n Lineal**, que muestra la tendencia general de los datos.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Cargar la base de datos del Titanic\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# Seleccionar solo columnas num√©ricas y eliminar valores nulos\n",
    "df = df[['fare', 'age', 'pclass', 'sibsp', 'parch']].dropna()\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b188d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir variables predictoras (X) y variable objetivo (y)\n",
    "X = df[['pclass', 'age']]\n",
    "y = df['fare']\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el modelo de Regresi√≥n Lineal\n",
    "modelo = LinearRegression()\n",
    "\n",
    "# Entrenar el modelo\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Coeficientes del modelo:\", modelo.coef_)\n",
    "print(\"Intercepto del modelo:\", modelo.intercept_)\n",
    "print(\"Error cuadr√°tico medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinaci√≥n (R¬≤):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80253d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una nueva variable: Total de familiares a bordo\n",
    "df[\"family_size\"] = df[\"sibsp\"] + df[\"parch\"]\n",
    "\n",
    "# Definir variables\n",
    "X = df[['family_size']]\n",
    "y = df['fare']\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "modelo_familia = LinearRegression()\n",
    "modelo_familia.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_familia = modelo_familia.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mse_familia = mean_squared_error(y_test, y_pred_familia)\n",
    "r2_familia = r2_score(y_test, y_pred_familia)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Coeficiente del modelo:\", modelo_familia.coef_[0])\n",
    "print(\"Intercepto:\", modelo_familia.intercept_)\n",
    "print(\"MSE:\", mse_familia)\n",
    "print(\"R¬≤:\", r2_familia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de regresi√≥n: Tarifa vs Edad\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(x=df['age'], y=df['fare'], alpha=0.5, label=\"Datos reales\")\n",
    "sns.lineplot(x=df['age'], y=modelo.predict(df[['pclass', 'age']]), color=\"red\", label=\"Regresi√≥n lineal\")\n",
    "plt.title(\"Relaci√≥n entre Edad y Tarifa del Boleto\")\n",
    "plt.xlabel(\"Edad\")\n",
    "plt.ylabel(\"Tarifa (Fare)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400ba64",
   "metadata": {},
   "source": [
    "# Distancias en Machine Learning\n",
    "\n",
    "En muchos algoritmos de machine learning (como KNN, clustering, reducci√≥n de dimensionalidad, etc.) es fundamental medir **qu√© tan cerca o lejos** est√°n dos puntos entre s√≠. Para eso, usamos **m√©tricas de distancia**.\n",
    "\n",
    "A continuaci√≥n, veremos 3 distancias comunes:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Distancia Euclidiana\n",
    "\n",
    "La **distancia euclidiana** es la m√°s com√∫n y se calcula como la ra√≠z cuadrada de la suma de las diferencias al cuadrado entre cada dimensi√≥n:\n",
    "\n",
    "$$\n",
    "d(p, q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \\dots + (p_n - q_n)^2}\n",
    "$$\n",
    "\n",
    "Se puede imaginar como una regla entre dos puntos en un espacio.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Distancia de Minkowski\n",
    "\n",
    "La **distancia de Minkowski** generaliza la euclidiana y la de Manhattan. Se define como:\n",
    "\n",
    "$$\n",
    "d(p, q) = \\left( \\sum_{i=1}^{n} |p_i - q_i|^r \\right)^{1/r}\n",
    "$$\n",
    "\n",
    "- Si \\( r = 1 \\) ‚Üí es la **distancia de Manhattan**.  \n",
    "- Si \\( r = 2 \\) ‚Üí es la **distancia euclidiana**.  \n",
    "- Se puede ajustar seg√∫n el problema.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Distancia de Mahalanobis\n",
    "\n",
    "Esta distancia tiene en cuenta la **distribuci√≥n de los datos** (correlaci√≥n y escala), por lo que es √∫til cuando las variables tienen distinta escala o est√°n correlacionadas.\n",
    "\n",
    "$$\n",
    "d(p, q) = \\sqrt{(p - q)^T \\, S^{-1} \\, (p - q)}\n",
    "$$\n",
    "\n",
    "- \\( S \\) es la **matriz de covarianza** de los datos.  \n",
    "- Penaliza m√°s las diferencias en dimensiones con baja varianza.  \n",
    "- Se usa mucho en detecci√≥n de outliers y an√°lisis multivariado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da7a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punto 1: [ 39.1  18.7 181. ]\n",
      "Punto 2: [ 39.   17.5 186. ]\n",
      "Distancia Euclidiana: 5.14\n",
      "Distancia de Minkowski (r=3): 5.02\n",
      "Distancia de Mahalanobis: 0.65\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cargar el dataset de ping√ºinos y seleccionar algunas columnas num√©ricas\n",
    "df = sns.load_dataset(\"penguins\").dropna()\n",
    "df = df[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm']]\n",
    "\n",
    "# Tomamos dos muestras cualquiera\n",
    "p1 = df.iloc[0].values\n",
    "p2 = df.iloc[50].values\n",
    "\n",
    "print(\"Punto 1:\", p1)\n",
    "print(\"Punto 2:\", p2)\n",
    "\n",
    "# 1. Distancia Euclidiana\n",
    "eucl = distance.euclidean(p1, p2)\n",
    "print(f\"Distancia Euclidiana: {eucl:.2f}\")\n",
    "\n",
    "# 2. Distancia de Minkowski (r = 3, por ejemplo)\n",
    "mink = distance.minkowski(p1, p2, 3)\n",
    "print(f\"Distancia de Minkowski (r=3): {mink:.2f}\")\n",
    "\n",
    "# 3. Distancia de Mahalanobis\n",
    "# Necesitamos estandarizar los datos primero\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "\n",
    "# Matriz de covarianza\n",
    "cov = np.cov(scaled_df, rowvar=False)\n",
    "inv_covmat = np.linalg.inv(cov)\n",
    "\n",
    "# Calcular distancia de Mahalanobis entre los mismos puntos (ya escalados)\n",
    "scaled_p1 = scaled_df[0]\n",
    "scaled_p2 = scaled_df[50]\n",
    "\n",
    "maha = distance.mahalanobis(scaled_p1, scaled_p2, inv_covmat)\n",
    "print(f\"Distancia de Mahalanobis: {maha:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ae002",
   "metadata": {},
   "source": [
    "### ¬øQu√© representan estos puntos?\n",
    "\n",
    "Cada punto representa las caracter√≠sticas de un ping√ºino:\n",
    "- `bill_length_mm`: Largo del pico en mil√≠metros.\n",
    "- `bill_depth_mm`: Profundidad del pico.\n",
    "- `flipper_length_mm`: Largo de la aleta.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretaci√≥n de las Distancias\n",
    "\n",
    "#### **Distancia Euclidiana: 5.14**\n",
    "- Es la distancia ‚Äúnormal‚Äù entre dos puntos en el espacio tridimensional definido por las 3 variables.\n",
    "- Es sensible a las unidades y escalas de las variables.\n",
    "- Aqu√≠ nos indica que los dos ping√ºinos est√°n **moderadamente separados** en ese espacio.\n",
    "\n",
    "#### **Distancia de Minkowski (r=3): 5.02**\n",
    "- Generaliza la euclidiana (que es el caso cuando r=2).\n",
    "- Al usar \\( r = 3 \\), se penalizan un poco m√°s las diferencias grandes.\n",
    "- El valor es **ligeramente menor**, indicando una penalizaci√≥n distinta al hacer el c√°lculo.\n",
    "\n",
    "#### **Distancia de Mahalanobis: 0.65**\n",
    "- Esta distancia **toma en cuenta la correlaci√≥n y la escala de los datos**.\n",
    "- Un valor bajo (como 0.65) indica que **aunque num√©ricamente las diferencias parecen grandes**, seg√∫n la distribuci√≥n de los datos **los puntos no est√°n tan alejados estad√≠sticamente**.\n",
    "- En otras palabras, est√°n cerca **en t√©rminos del comportamiento de la poblaci√≥n**.\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øQu√© aprendemos?\n",
    "\n",
    "- Si solo usamos distancia euclidiana, **podemos sobrevalorar la diferencia** si no consideramos la correlaci√≥n entre variables.\n",
    "- Mahalanobis **normaliza las variables** y ajusta seg√∫n la distribuci√≥n general del dataset.\n",
    "- Es ideal cuando las variables est√°n en **diferentes escalas** o tienen correlaciones (como suele pasar en datos reales).\n",
    "\n",
    "---\n",
    "\n",
    "### Reflexi√≥n\n",
    "\n",
    "Dos puntos pueden estar lejos seg√∫n una distancia, pero **cerca seg√∫n otra**, dependiendo de c√≥mo consideramos la estructura interna de los datos.\n",
    "\n",
    "Esto es clave al usar algoritmos como **KNN**, **detector de outliers** o **modelos de clustering**, donde **la elecci√≥n de la distancia importa mucho**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4f9c9",
   "metadata": {},
   "source": [
    "# Clasificador K-Nearest Neighbors (KNN)\n",
    "\n",
    "## ¬øQu√© es KNN?\n",
    "\n",
    "KNN (K-Nearest Neighbors) es un algoritmo de **clasificaci√≥n supervisada** (tambi√©n puede usarse para regresi√≥n). Su idea principal es **predecir la clase de un dato nuevo** observando las clases de sus **k vecinos m√°s cercanos** en el espacio de caracter√≠sticas.\n",
    "\n",
    "---\n",
    "\n",
    "## ¬øC√≥mo funciona?\n",
    "\n",
    "1. Se calcula la distancia entre el nuevo punto y todos los puntos del conjunto de entrenamiento (usualmente con distancia **euclidiana**).\n",
    "2. Se seleccionan los **k puntos m√°s cercanos** (los vecinos).\n",
    "3. Se vota por la clase m√°s frecuente entre esos k vecinos.\n",
    "4. Se asigna esa clase al nuevo punto.\n",
    "\n",
    "---\n",
    "\n",
    "## ¬øQu√© tan importante es la distancia?\n",
    "\n",
    "Muy importante. El algoritmo depende completamente de **medir distancias correctamente** entre puntos. Por eso se puede usar:\n",
    "\n",
    "- Distancia Euclidiana\n",
    "- Distancia de Manhattan\n",
    "- Distancia de Mahalanobis (menos com√∫n en KNN, pero posible)\n",
    "\n",
    "---\n",
    "\n",
    "## ¬øC√≥mo elegir k?\n",
    "\n",
    "- Si **k es muy peque√±o**, el modelo puede ser sensible al ruido (sobreajuste).\n",
    "- Si **k es muy grande**, puede volverse muy general (subajuste).\n",
    "- Se recomienda probar varios valores de k y usar **validaci√≥n cruzada**.\n",
    "\n",
    "---\n",
    "\n",
    "## Ventajas y desventajas\n",
    "\n",
    "(Ventaja) Simple de entender y aplicar.  \n",
    "(Ventaja) No necesita entrenamiento (lazy learning).  \n",
    "(Desventaja) Lento si hay muchos datos.  \n",
    "(Desventaja) Sensible a escala y ruido.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c67018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Matriz de Confusi√≥n:\n",
      " [[31  0  0]\n",
      " [ 1 12  0]\n",
      " [ 0  0 23]]\n",
      "\n",
      " Reporte de Clasificaci√≥n:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.97      1.00      0.98        31\n",
      "   Chinstrap       1.00      0.92      0.96        13\n",
      "      Gentoo       1.00      1.00      1.00        23\n",
      "\n",
      "    accuracy                           0.99        67\n",
      "   macro avg       0.99      0.97      0.98        67\n",
      "weighted avg       0.99      0.99      0.98        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Cargar dataset de penguins\n",
    "df = sns.load_dataset(\"penguins\").dropna()\n",
    "\n",
    "# Variables predictoras (X) y clase a predecir (y)\n",
    "X = df[['bill_length_mm', 'flipper_length_mm']]\n",
    "y = df['species']\n",
    "\n",
    "# Escalar los datos (importante para KNN)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo KNN con k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\" Matriz de Confusi√≥n:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n Reporte de Clasificaci√≥n:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f4ecd",
   "metadata": {},
   "source": [
    "### Entonces como una forma simple de ver el algoritmo\n",
    "\n",
    "1. Selecci√≥n del valor √≥ptimo de K: K representa el n√∫mero de vecinos\n",
    "m√°s cercanos que deben considerarse al realizar una predicci√≥n.\n",
    "\n",
    "2. C√°lculo de la distancia Para medir la similitud entre el objetivo y los\n",
    "puntos de datos de entrenamiento, se utiliza la distancia euclidiana. Se\n",
    "calcula la distancia entre cada uno de los puntos de datos en el\n",
    "conjunto de datos y el punto objetivo.\n",
    "\n",
    "3. Encontrar los vecinos m√°s cercanos Los k puntos de datos con las\n",
    "distancias m√°s peque√±as al punto objetivo son los vecinos m√°s\n",
    "cercanos.\n",
    "\n",
    "4. Votaci√≥n para clasificaci√≥n o tomar el promedio para regresi√≥n: En el\n",
    "problema de clasificaci√≥n, las etiquetas de clase se determinan\n",
    "realizando una votaci√≥n mayoritaria. La clase con m√°s ocurrencias\n",
    "entre los vecinos se convierte en la clase predicha para el punto de\n",
    "datos objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b1ea8",
   "metadata": {},
   "source": [
    "## Se recomienda tomar **k impar** para evitar empates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c36ab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8395e33",
   "metadata": {},
   "source": [
    "# ¬øQu√© significa escalar o normalizar los datos?\n",
    "\n",
    "Cuando hablamos de **escalar** o **normalizar** los datos en Machine Learning, nos referimos a **transformar las variables num√©ricas** para que tengan una escala similar.\n",
    "\n",
    "---\n",
    "\n",
    "## ¬øPor qu√© es importante?\n",
    "\n",
    "Muchos algoritmos (como **KNN**, **SVM**, **Regresi√≥n log√≠stica**, etc.) dependen de c√°lculos de **distancias** o **gradientes**.  \n",
    "Si una variable tiene valores mucho m√°s grandes que otra, puede **dominar** la predicci√≥n, incluso si no es la m√°s importante.\n",
    "\n",
    "**Ejemplo:**  \n",
    "- `flipper_length_mm` puede ir de **170 a 230**  \n",
    "- `bill_depth_mm` puede ir de **13 a 21**\n",
    "\n",
    "Sin escalar, la distancia entre dos ping√ºinos va a estar influenciada mucho m√°s por `flipper_length_mm`, solo porque sus n√∫meros son m√°s grandes.\n",
    "\n",
    "---\n",
    "\n",
    "## T√©cnicas m√°s comunes\n",
    "\n",
    "### 1. Normalizaci√≥n Min-Max  \n",
    "Convierte los datos a un rango entre 0 y 1:  \n",
    "$$\n",
    "X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "$$\n",
    "\n",
    "### 2. Estandarizaci√≥n (Z-score scaling)  \n",
    "Convierte los datos para que tengan **media 0** y **desviaci√≥n est√°ndar 1**:  \n",
    "$$\n",
    "X_{std} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "Es la t√©cnica que usamos con `StandardScaler()` de sklearn.\n",
    "\n",
    "---\n",
    "\n",
    "## ¬øCu√°ndo es obligatorio escalar?\n",
    "\n",
    "- (S√≠) Algoritmos basados en distancias (**KNN**, **SVM**, **Clustering**)\n",
    "- (S√≠) Algoritmos que usan derivadas (**Regresi√≥n log√≠stica**, **Redes neuronales**)\n",
    "- (No) No es necesario en modelos basados en √°rboles (**Random Forest**, **XGBoost**)\n",
    "\n",
    "---\n",
    "\n",
    "## En sklearn se hace as√≠:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d20f8",
   "metadata": {},
   "source": [
    "Dentro del siguiente link podr√°n visualizar como se utiliza KNN, -> [\"C√≥mo funciona KNN\"](https://youtu.be/zeFt_JCA3b4?si=uVV7XqJOxQNRcvHG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a1ddc9",
   "metadata": {},
   "source": [
    "# Regresi√≥n Log√≠stica ‚Äì Clasificaci√≥n Supervisada\n",
    "\n",
    "## ¬øQu√© es la Regresi√≥n Log√≠stica?\n",
    "\n",
    "La **regresi√≥n log√≠stica** es un modelo de **clasificaci√≥n supervisada** que se usa para predecir **probabilidades** de pertenecer a una clase.  \n",
    "Aunque se llama \"regresi√≥n\", se usa para **clasificaci√≥n binaria** (S√≠/No, 0/1, Verdadero/Falso).\n",
    "\n",
    "---\n",
    "\n",
    "## ¬øC√≥mo funciona?\n",
    "\n",
    "1. A diferencia de la regresi√≥n lineal, no predice directamente un n√∫mero real, sino la **probabilidad** de que un ejemplo pertenezca a la clase positiva (por ejemplo, \"sobrevive\").\n",
    "2. Utiliza la **funci√≥n sigmoide** para transformar la salida en un valor entre 0 y 1:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "donde $z = w_0 + w_1x_1 + w_2x_2 + \\dots + w_nx_n $\n",
    "\n",
    "---\n",
    "\n",
    "## ¬øC√≥mo se interpreta?\n",
    "\n",
    "- Si $ \\sigma(z) \\geq 0.5 $, el modelo predice **clase 1**\n",
    "- Si $ \\sigma(z) < 0.5 $, el modelo predice **clase 0**\n",
    "\n",
    ">  Tambi√©n puedes cambiar ese umbral si lo necesitas (por ejemplo: 0.6, 0.3...).\n",
    "\n",
    "---\n",
    "\n",
    "## ¬øCu√°ndo usarla?\n",
    "\n",
    "- Problemas de **clasificaci√≥n binaria** (spam/no spam, enfermo/sano, etc.).\n",
    "- Cuando quieres obtener una **probabilidad** y no solo una clase.\n",
    "- Cuando tus variables de entrada son num√©ricas o categ√≥ricas.\n",
    "\n",
    "---\n",
    "\n",
    "## Ventajas\n",
    "\n",
    "(Ventaja) Modelo simple, r√°pido y f√°cil de interpretar.  \n",
    "(Ventaja) Bueno para datos linealmente separables.  \n",
    "(Ventaja) Devuelve probabilidades, lo que permite controlar umbrales.  \n",
    "\n",
    "(Desventaja) No funciona bien cuando los datos **no se separan linealmente**.  \n",
    "(Desventaja) Sensible a **outliers y escala** ‚Üí por eso es buena idea escalar los datos.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a93c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Matriz de Confusi√≥n:\n",
      " [[35  1]\n",
      " [ 0 31]]\n",
      "\n",
      " Reporte de Clasificaci√≥n:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        36\n",
      "           1       0.97      1.00      0.98        31\n",
      "\n",
      "    accuracy                           0.99        67\n",
      "   macro avg       0.98      0.99      0.99        67\n",
      "weighted avg       0.99      0.99      0.99        67\n",
      "\n",
      "\n",
      "üîç Probabilidades del primer ping√ºino en test:\n",
      " [0.01623011 0.98376989]\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Cargar dataset de ping√ºinos\n",
    "df = sns.load_dataset(\"penguins\").dropna()\n",
    "\n",
    "# Clasificaci√≥n binaria: ¬øes 'Adelie' o no?\n",
    "df['is_adelie'] = (df['species'] == 'Adelie').astype(int)\n",
    "\n",
    "# Variables predictoras\n",
    "X = df[['bill_length_mm', 'flipper_length_mm']]\n",
    "y = df['is_adelie']\n",
    "\n",
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "modelo = LogisticRegression()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\" Matriz de Confusi√≥n:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n Reporte de Clasificaci√≥n:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Mostrar probabilidades predichas\n",
    "probas = modelo.predict_proba(X_test)\n",
    "print(\"\\nüîç Probabilidades del primer ping√ºino en test:\\n\", probas[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec34baf",
   "metadata": {},
   "source": [
    "\n",
    "### Matriz de Confusi√≥n\n",
    "\n",
    "- **35** ping√ºinos que **no eran Adelie (clase 0)** fueron correctamente clasificados.\n",
    "- **31** ping√ºinos **Adelie (clase 1)** fueron correctamente clasificados.\n",
    "- Solo **1 error**: un ping√ºino no-Adelie fue clasificado como Adelie (falso positivo).\n",
    "- No hubo falsos negativos (no se confundi√≥ ning√∫n Adelie como no-Adelie).\n",
    "\n",
    "---\n",
    "\n",
    "###  Reporte de Clasificaci√≥n\n",
    "- **Precisi√≥n (Precision):**\n",
    "  - Clase 0: Todas las veces que el modelo dijo \"no es Adelie\", acert√≥ (100%).\n",
    "  - Clase 1: Cuando dijo \"es Adelie\", acert√≥ el 97% de las veces.\n",
    "  \n",
    "- **Recall:**\n",
    "  - Clase 0: Detect√≥ correctamente el 97% de los casos no-Adelie.\n",
    "  - Clase 1: Detect√≥ correctamente el 100% de los casos Adelie.\n",
    "\n",
    "- **F1-score:** Equilibrio entre precisi√≥n y recall. Ambos son altos (‚âà0.98 - 0.99), lo que indica un excelente desempe√±o.\n",
    "\n",
    "---\n",
    "\n",
    "### Accuracy General\n",
    "\n",
    "- El modelo clasific√≥ correctamente **66 de 67 ping√ºinos**, logrando una **exactitud de 99%**.\n",
    "- Esto significa que el modelo **generaliza bien** y tiene muy buen rendimiento en este problema de clasificaci√≥n binaria.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusi√≥n\n",
    "\n",
    "- La regresi√≥n log√≠stica funcion√≥ **de forma excelente** para predecir si un ping√ºino es de la especie Adelie.\n",
    "- El modelo es confiable y balanceado.\n",
    "- Solo cometi√≥ **un error**, lo que es normal en la pr√°ctica.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
