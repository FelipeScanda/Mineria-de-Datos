{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b8c60c",
   "metadata": {},
   "source": [
    "## Ayudantía (3) Extra\n",
    "\n",
    "### Profesor: Luis Cossio\n",
    "### Ayudante: Gabriel Díaz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26dd111",
   "metadata": {},
   "source": [
    "# **Regresión Lineal con el Dataset del Titanic**  \n",
    "\n",
    "## **¿Qué es la Regresión Lineal?**  \n",
    "La **Regresión Lineal** es un método estadístico que permite modelar la relación entre una variable dependiente (objetivo) y una o más variables independientes (predictoras). Se basa en la ecuación:  \n",
    "\n",
    "$$ y = b_0 + b_1x_1 + b_2x_2 + \\dots + b_nx_n $$  \n",
    "\n",
    "Donde:  \n",
    "- \\( y \\) es la variable objetivo.  \n",
    "- $x_1, x_2, ... x_n $ son las variables predictoras.  \n",
    "- $ b_0 $ es el **intercepto** (valor de \\( y \\) cuando todas las \\( x \\) son 0).  \n",
    "- $ b_1, b_2, ..., b_n $ son los **coeficientes** de cada variable.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Preparación de los Datos**  \n",
    "### **Carga del dataset y selección de variables**  \n",
    "En este análisis, usamos el dataset **Titanic**, que contiene información sobre los pasajeros, como la tarifa del boleto (`fare`), la edad (`age`), la clase del boleto (`pclass`), y el número de familiares a bordo (`sibsp` y `parch`).  \n",
    "\n",
    "Para construir el modelo, seleccionamos las siguientes variables:  \n",
    "\n",
    "- **Variable objetivo:** `fare` (Tarifa del boleto)  \n",
    "- **Variables predictoras:**  \n",
    "  - `pclass` (Clase del boleto: 1, 2, 3)  \n",
    "  - `age` (Edad del pasajero)  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. División en Datos de Entrenamiento y Prueba**  \n",
    "Para evaluar el rendimiento del modelo, se divide el dataset en dos partes:  \n",
    "\n",
    "1. **Datos de Entrenamiento (80%)**: Se usan para ajustar el modelo.  \n",
    "2. **Datos de Prueba (20%)**: Se usan para evaluar qué tan bien predice el modelo en datos no vistos.  \n",
    "\n",
    "Esto se debe a que es un algoritmo supervisado, los datos de entrenamiento y de prueba.\n",
    "\n",
    "Usamos la función `train_test_split()` de `sklearn.model_selection`, que separa aleatoriamente los datos en estas dos partes.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Creación y Entrenamiento del Modelo**  \n",
    "1. Se crea una instancia del modelo de **Regresión Lineal** con `LinearRegression()`.  \n",
    "2. Se entrena el modelo con los datos de entrenamiento usando `.fit(X_train, y_train)`.  \n",
    "\n",
    "Durante el entrenamiento, el modelo **ajusta los coeficientes $b_1, b_2, \\dots $** para minimizar el error entre las predicciones y los valores reales.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Predicción y Evaluación del Modelo**  \n",
    "Después del entrenamiento, usamos `.predict(X_test)` para hacer predicciones en los datos de prueba.  \n",
    "\n",
    "Para evaluar el modelo, calculamos:  \n",
    "- **Error Cuadrático Medio (MSE):**  \n",
    "  \n",
    "  $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "  - Mide cuánto se desvían las predicciones de los valores reales.  \n",
    "  - Valores menores indican mejor ajuste.  \n",
    "\n",
    "- **Coeficiente de Determinación (\\( R^2 \\)):**  \n",
    "\n",
    "  $$ R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} $$ \n",
    "  - Indica qué porcentaje de la variabilidad en `fare` es explicado por las variables predictoras.  \n",
    "  - Valores cercanos a 1 indican un buen ajuste.  \n",
    "\n",
    "Dependiendo del tipo de problema, usamos diferentes métricas de evaluación. Para regresión es distinto que para clasificación.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Visualización de la Regresión**  \n",
    "Para interpretar los resultados, se grafica la relación entre **Edad (`age`) y Tarifa (`fare`)**.  \n",
    "Se superpone la línea de **Regresión Lineal**, que muestra la tendencia general de los datos.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Cargar la base de datos del Titanic\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# Seleccionar solo columnas numéricas y eliminar valores nulos\n",
    "df = df[['fare', 'age', 'pclass', 'sibsp', 'parch']].dropna()\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b188d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir variables predictoras (X) y variable objetivo (y)\n",
    "X = df[['pclass', 'age']]\n",
    "y = df['fare']\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el modelo de Regresión Lineal\n",
    "modelo = LinearRegression()\n",
    "\n",
    "# Entrenar el modelo\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Coeficientes del modelo:\", modelo.coef_)\n",
    "print(\"Intercepto del modelo:\", modelo.intercept_)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Coeficiente de determinación (R²):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80253d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una nueva variable: Total de familiares a bordo\n",
    "df[\"family_size\"] = df[\"sibsp\"] + df[\"parch\"]\n",
    "\n",
    "# Definir variables\n",
    "X = df[['family_size']]\n",
    "y = df['fare']\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "modelo_familia = LinearRegression()\n",
    "modelo_familia.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_familia = modelo_familia.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mse_familia = mean_squared_error(y_test, y_pred_familia)\n",
    "r2_familia = r2_score(y_test, y_pred_familia)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Coeficiente del modelo:\", modelo_familia.coef_[0])\n",
    "print(\"Intercepto:\", modelo_familia.intercept_)\n",
    "print(\"MSE:\", mse_familia)\n",
    "print(\"R²:\", r2_familia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de regresión: Tarifa vs Edad\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(x=df['age'], y=df['fare'], alpha=0.5, label=\"Datos reales\")\n",
    "sns.lineplot(x=df['age'], y=modelo.predict(df[['pclass', 'age']]), color=\"red\", label=\"Regresión lineal\")\n",
    "plt.title(\"Relación entre Edad y Tarifa del Boleto\")\n",
    "plt.xlabel(\"Edad\")\n",
    "plt.ylabel(\"Tarifa (Fare)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400ba64",
   "metadata": {},
   "source": [
    "# Distancias en Machine Learning\n",
    "\n",
    "En muchos algoritmos de machine learning (como KNN, clustering, reducción de dimensionalidad, etc.) es fundamental medir **qué tan cerca o lejos** están dos puntos entre sí. Para eso, usamos **métricas de distancia**.\n",
    "\n",
    "A continuación, veremos 3 distancias comunes:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Distancia Euclidiana\n",
    "\n",
    "La **distancia euclidiana** es la más común y se calcula como la raíz cuadrada de la suma de las diferencias al cuadrado entre cada dimensión:\n",
    "\n",
    "$$\n",
    "d(p, q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \\dots + (p_n - q_n)^2}\n",
    "$$\n",
    "\n",
    "Se puede imaginar como una regla entre dos puntos en un espacio.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Distancia de Minkowski\n",
    "\n",
    "La **distancia de Minkowski** generaliza la euclidiana y la de Manhattan. Se define como:\n",
    "\n",
    "$$\n",
    "d(p, q) = \\left( \\sum_{i=1}^{n} |p_i - q_i|^r \\right)^{1/r}\n",
    "$$\n",
    "\n",
    "- Si \\( r = 1 \\) → es la **distancia de Manhattan**.  \n",
    "- Si \\( r = 2 \\) → es la **distancia euclidiana**.  \n",
    "- Se puede ajustar según el problema.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Distancia de Mahalanobis\n",
    "\n",
    "Esta distancia tiene en cuenta la **distribución de los datos** (correlación y escala), por lo que es útil cuando las variables tienen distinta escala o están correlacionadas.\n",
    "\n",
    "$$\n",
    "d(p, q) = \\sqrt{(p - q)^T \\, S^{-1} \\, (p - q)}\n",
    "$$\n",
    "\n",
    "- \\( S \\) es la **matriz de covarianza** de los datos.  \n",
    "- Penaliza más las diferencias en dimensiones con baja varianza.  \n",
    "- Se usa mucho en detección de outliers y análisis multivariado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da7a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punto 1: [ 39.1  18.7 181. ]\n",
      "Punto 2: [ 39.   17.5 186. ]\n",
      "Distancia Euclidiana: 5.14\n",
      "Distancia de Minkowski (r=3): 5.02\n",
      "Distancia de Mahalanobis: 0.65\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cargar el dataset de pingüinos y seleccionar algunas columnas numéricas\n",
    "df = sns.load_dataset(\"penguins\").dropna()\n",
    "df = df[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm']]\n",
    "\n",
    "# Tomamos dos muestras cualquiera\n",
    "p1 = df.iloc[0].values\n",
    "p2 = df.iloc[50].values\n",
    "\n",
    "print(\"Punto 1:\", p1)\n",
    "print(\"Punto 2:\", p2)\n",
    "\n",
    "# 1. Distancia Euclidiana\n",
    "eucl = distance.euclidean(p1, p2)\n",
    "print(f\"Distancia Euclidiana: {eucl:.2f}\")\n",
    "\n",
    "# 2. Distancia de Minkowski (r = 3, por ejemplo)\n",
    "mink = distance.minkowski(p1, p2, 3)\n",
    "print(f\"Distancia de Minkowski (r=3): {mink:.2f}\")\n",
    "\n",
    "# 3. Distancia de Mahalanobis\n",
    "# Necesitamos estandarizar los datos primero\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "\n",
    "# Matriz de covarianza\n",
    "cov = np.cov(scaled_df, rowvar=False)\n",
    "inv_covmat = np.linalg.inv(cov)\n",
    "\n",
    "# Calcular distancia de Mahalanobis entre los mismos puntos (ya escalados)\n",
    "scaled_p1 = scaled_df[0]\n",
    "scaled_p2 = scaled_df[50]\n",
    "\n",
    "maha = distance.mahalanobis(scaled_p1, scaled_p2, inv_covmat)\n",
    "print(f\"Distancia de Mahalanobis: {maha:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ae002",
   "metadata": {},
   "source": [
    "### ¿Qué representan estos puntos?\n",
    "\n",
    "Cada punto representa las características de un pingüino:\n",
    "- `bill_length_mm`: Largo del pico en milímetros.\n",
    "- `bill_depth_mm`: Profundidad del pico.\n",
    "- `flipper_length_mm`: Largo de la aleta.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretación de las Distancias\n",
    "\n",
    "#### **Distancia Euclidiana: 5.14**\n",
    "- Es la distancia “normal” entre dos puntos en el espacio tridimensional definido por las 3 variables.\n",
    "- Es sensible a las unidades y escalas de las variables.\n",
    "- Aquí nos indica que los dos pingüinos están **moderadamente separados** en ese espacio.\n",
    "\n",
    "#### **Distancia de Minkowski (r=3): 5.02**\n",
    "- Generaliza la euclidiana (que es el caso cuando r=2).\n",
    "- Al usar \\( r = 3 \\), se penalizan un poco más las diferencias grandes.\n",
    "- El valor es **ligeramente menor**, indicando una penalización distinta al hacer el cálculo.\n",
    "\n",
    "#### **Distancia de Mahalanobis: 0.65**\n",
    "- Esta distancia **toma en cuenta la correlación y la escala de los datos**.\n",
    "- Un valor bajo (como 0.65) indica que **aunque numéricamente las diferencias parecen grandes**, según la distribución de los datos **los puntos no están tan alejados estadísticamente**.\n",
    "- En otras palabras, están cerca **en términos del comportamiento de la población**.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Qué aprendemos?\n",
    "\n",
    "- Si solo usamos distancia euclidiana, **podemos sobrevalorar la diferencia** si no consideramos la correlación entre variables.\n",
    "- Mahalanobis **normaliza las variables** y ajusta según la distribución general del dataset.\n",
    "- Es ideal cuando las variables están en **diferentes escalas** o tienen correlaciones (como suele pasar en datos reales).\n",
    "\n",
    "---\n",
    "\n",
    "### Reflexión\n",
    "\n",
    "Dos puntos pueden estar lejos según una distancia, pero **cerca según otra**, dependiendo de cómo consideramos la estructura interna de los datos.\n",
    "\n",
    "Esto es clave al usar algoritmos como **KNN**, **detector de outliers** o **modelos de clustering**, donde **la elección de la distancia importa mucho**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4f9c9",
   "metadata": {},
   "source": [
    "# Clasificador K-Nearest Neighbors (KNN)\n",
    "\n",
    "## ¿Qué es KNN?\n",
    "\n",
    "KNN (K-Nearest Neighbors) es un algoritmo de **clasificación supervisada** (también puede usarse para regresión). Su idea principal es **predecir la clase de un dato nuevo** observando las clases de sus **k vecinos más cercanos** en el espacio de características.\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Cómo funciona?\n",
    "\n",
    "1. Se calcula la distancia entre el nuevo punto y todos los puntos del conjunto de entrenamiento (usualmente con distancia **euclidiana**).\n",
    "2. Se seleccionan los **k puntos más cercanos** (los vecinos).\n",
    "3. Se vota por la clase más frecuente entre esos k vecinos.\n",
    "4. Se asigna esa clase al nuevo punto.\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Qué tan importante es la distancia?\n",
    "\n",
    "Muy importante. El algoritmo depende completamente de **medir distancias correctamente** entre puntos. Por eso se puede usar:\n",
    "\n",
    "- Distancia Euclidiana\n",
    "- Distancia de Manhattan\n",
    "- Distancia de Mahalanobis (menos común en KNN, pero posible)\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Cómo elegir k?\n",
    "\n",
    "- Si **k es muy pequeño**, el modelo puede ser sensible al ruido (sobreajuste).\n",
    "- Si **k es muy grande**, puede volverse muy general (subajuste).\n",
    "- Se recomienda probar varios valores de k y usar **validación cruzada**.\n",
    "\n",
    "---\n",
    "\n",
    "## Ventajas y desventajas\n",
    "\n",
    "(Ventaja) Simple de entender y aplicar.  \n",
    "(Ventaja) No necesita entrenamiento (lazy learning).  \n",
    "(Desventaja) Lento si hay muchos datos.  \n",
    "(Desventaja) Sensible a escala y ruido.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c67018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Matriz de Confusión:\n",
      " [[31  0  0]\n",
      " [ 1 12  0]\n",
      " [ 0  0 23]]\n",
      "\n",
      " Reporte de Clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.97      1.00      0.98        31\n",
      "   Chinstrap       1.00      0.92      0.96        13\n",
      "      Gentoo       1.00      1.00      1.00        23\n",
      "\n",
      "    accuracy                           0.99        67\n",
      "   macro avg       0.99      0.97      0.98        67\n",
      "weighted avg       0.99      0.99      0.98        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Cargar dataset de penguins\n",
    "df = sns.load_dataset(\"penguins\").dropna()\n",
    "\n",
    "# Variables predictoras (X) y clase a predecir (y)\n",
    "X = df[['bill_length_mm', 'flipper_length_mm']]\n",
    "y = df['species']\n",
    "\n",
    "# Escalar los datos (importante para KNN)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo KNN con k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\" Matriz de Confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n Reporte de Clasificación:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f4ecd",
   "metadata": {},
   "source": [
    "### Entonces como una forma simple de ver el algoritmo\n",
    "\n",
    "1. Selección del valor óptimo de K: K representa el número de vecinos\n",
    "más cercanos que deben considerarse al realizar una predicción.\n",
    "\n",
    "2. Cálculo de la distancia Para medir la similitud entre el objetivo y los\n",
    "puntos de datos de entrenamiento, se utiliza la distancia euclidiana. Se\n",
    "calcula la distancia entre cada uno de los puntos de datos en el\n",
    "conjunto de datos y el punto objetivo.\n",
    "\n",
    "3. Encontrar los vecinos más cercanos Los k puntos de datos con las\n",
    "distancias más pequeñas al punto objetivo son los vecinos más\n",
    "cercanos.\n",
    "\n",
    "4. Votación para clasificación o tomar el promedio para regresión: En el\n",
    "problema de clasificación, las etiquetas de clase se determinan\n",
    "realizando una votación mayoritaria. La clase con más ocurrencias\n",
    "entre los vecinos se convierte en la clase predicha para el punto de\n",
    "datos objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b1ea8",
   "metadata": {},
   "source": [
    "## Se recomienda tomar **k impar** para evitar empates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c36ab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8395e33",
   "metadata": {},
   "source": [
    "# ¿Qué significa escalar o normalizar los datos?\n",
    "\n",
    "Cuando hablamos de **escalar** o **normalizar** los datos en Machine Learning, nos referimos a **transformar las variables numéricas** para que tengan una escala similar.\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Por qué es importante?\n",
    "\n",
    "Muchos algoritmos (como **KNN**, **SVM**, **Regresión logística**, etc.) dependen de cálculos de **distancias** o **gradientes**.  \n",
    "Si una variable tiene valores mucho más grandes que otra, puede **dominar** la predicción, incluso si no es la más importante.\n",
    "\n",
    "**Ejemplo:**  \n",
    "- `flipper_length_mm` puede ir de **170 a 230**  \n",
    "- `bill_depth_mm` puede ir de **13 a 21**\n",
    "\n",
    "Sin escalar, la distancia entre dos pingüinos va a estar influenciada mucho más por `flipper_length_mm`, solo porque sus números son más grandes.\n",
    "\n",
    "---\n",
    "\n",
    "## Técnicas más comunes\n",
    "\n",
    "### 1. Normalización Min-Max  \n",
    "Convierte los datos a un rango entre 0 y 1:  \n",
    "$$\n",
    "X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "$$\n",
    "\n",
    "### 2. Estandarización (Z-score scaling)  \n",
    "Convierte los datos para que tengan **media 0** y **desviación estándar 1**:  \n",
    "$$\n",
    "X_{std} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "Es la técnica que usamos con `StandardScaler()` de sklearn.\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Cuándo es obligatorio escalar?\n",
    "\n",
    "- (Sí) Algoritmos basados en distancias (**KNN**, **SVM**, **Clustering**)\n",
    "- (Sí) Algoritmos que usan derivadas (**Regresión logística**, **Redes neuronales**)\n",
    "- (No) No es necesario en modelos basados en árboles (**Random Forest**, **XGBoost**)\n",
    "\n",
    "---\n",
    "\n",
    "## En sklearn se hace así:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d20f8",
   "metadata": {},
   "source": [
    "Dentro del siguiente link podrán visualizar como se utiliza KNN, -> [\"Cómo funciona KNN\"](https://youtu.be/zeFt_JCA3b4?si=uVV7XqJOxQNRcvHG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a1ddc9",
   "metadata": {},
   "source": [
    "# Regresión Logística – Clasificación Supervisada\n",
    "\n",
    "## ¿Qué es la Regresión Logística?\n",
    "\n",
    "La **regresión logística** es un modelo de **clasificación supervisada** que se usa para predecir **probabilidades** de pertenecer a una clase.  \n",
    "Aunque se llama \"regresión\", se usa para **clasificación binaria** (Sí/No, 0/1, Verdadero/Falso).\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Cómo funciona?\n",
    "\n",
    "1. A diferencia de la regresión lineal, no predice directamente un número real, sino la **probabilidad** de que un ejemplo pertenezca a la clase positiva (por ejemplo, \"sobrevive\").\n",
    "2. Utiliza la **función sigmoide** para transformar la salida en un valor entre 0 y 1:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "donde $z = w_0 + w_1x_1 + w_2x_2 + \\dots + w_nx_n $\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Cómo se interpreta?\n",
    "\n",
    "- Si $ \\sigma(z) \\geq 0.5 $, el modelo predice **clase 1**\n",
    "- Si $ \\sigma(z) < 0.5 $, el modelo predice **clase 0**\n",
    "\n",
    ">  También puedes cambiar ese umbral si lo necesitas (por ejemplo: 0.6, 0.3...).\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Cuándo usarla?\n",
    "\n",
    "- Problemas de **clasificación binaria** (spam/no spam, enfermo/sano, etc.).\n",
    "- Cuando quieres obtener una **probabilidad** y no solo una clase.\n",
    "- Cuando tus variables de entrada son numéricas o categóricas.\n",
    "\n",
    "---\n",
    "\n",
    "## Ventajas\n",
    "\n",
    "(Ventaja) Modelo simple, rápido y fácil de interpretar.  \n",
    "(Ventaja) Bueno para datos linealmente separables.  \n",
    "(Ventaja) Devuelve probabilidades, lo que permite controlar umbrales.  \n",
    "\n",
    "(Desventaja) No funciona bien cuando los datos **no se separan linealmente**.  \n",
    "(Desventaja) Sensible a **outliers y escala** → por eso es buena idea escalar los datos.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a93c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Matriz de Confusión:\n",
      " [[35  1]\n",
      " [ 0 31]]\n",
      "\n",
      " Reporte de Clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        36\n",
      "           1       0.97      1.00      0.98        31\n",
      "\n",
      "    accuracy                           0.99        67\n",
      "   macro avg       0.98      0.99      0.99        67\n",
      "weighted avg       0.99      0.99      0.99        67\n",
      "\n",
      "\n",
      "🔍 Probabilidades del primer pingüino en test:\n",
      " [0.01623011 0.98376989]\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Cargar dataset de pingüinos\n",
    "df = sns.load_dataset(\"penguins\").dropna()\n",
    "\n",
    "# Clasificación binaria: ¿es 'Adelie' o no?\n",
    "df['is_adelie'] = (df['species'] == 'Adelie').astype(int)\n",
    "\n",
    "# Variables predictoras\n",
    "X = df[['bill_length_mm', 'flipper_length_mm']]\n",
    "y = df['is_adelie']\n",
    "\n",
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "modelo = LogisticRegression()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\" Matriz de Confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n Reporte de Clasificación:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Mostrar probabilidades predichas\n",
    "probas = modelo.predict_proba(X_test)\n",
    "print(\"\\n🔍 Probabilidades del primer pingüino en test:\\n\", probas[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec34baf",
   "metadata": {},
   "source": [
    "\n",
    "### Matriz de Confusión\n",
    "\n",
    "- **35** pingüinos que **no eran Adelie (clase 0)** fueron correctamente clasificados.\n",
    "- **31** pingüinos **Adelie (clase 1)** fueron correctamente clasificados.\n",
    "- Solo **1 error**: un pingüino no-Adelie fue clasificado como Adelie (falso positivo).\n",
    "- No hubo falsos negativos (no se confundió ningún Adelie como no-Adelie).\n",
    "\n",
    "---\n",
    "\n",
    "###  Reporte de Clasificación\n",
    "- **Precisión (Precision):**\n",
    "  - Clase 0: Todas las veces que el modelo dijo \"no es Adelie\", acertó (100%).\n",
    "  - Clase 1: Cuando dijo \"es Adelie\", acertó el 97% de las veces.\n",
    "  \n",
    "- **Recall:**\n",
    "  - Clase 0: Detectó correctamente el 97% de los casos no-Adelie.\n",
    "  - Clase 1: Detectó correctamente el 100% de los casos Adelie.\n",
    "\n",
    "- **F1-score:** Equilibrio entre precisión y recall. Ambos son altos (≈0.98 - 0.99), lo que indica un excelente desempeño.\n",
    "\n",
    "---\n",
    "\n",
    "### Accuracy General\n",
    "\n",
    "- El modelo clasificó correctamente **66 de 67 pingüinos**, logrando una **exactitud de 99%**.\n",
    "- Esto significa que el modelo **generaliza bien** y tiene muy buen rendimiento en este problema de clasificación binaria.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "- La regresión logística funcionó **de forma excelente** para predecir si un pingüino es de la especie Adelie.\n",
    "- El modelo es confiable y balanceado.\n",
    "- Solo cometió **un error**, lo que es normal en la práctica.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
